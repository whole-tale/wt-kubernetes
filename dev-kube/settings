
#
# Cluster type. Supported values are:
#	"gke": Google Kubernetes Engine
#	"minikube": Local cluster with minikube (most likely not working)
#	"none": if you want to manually configure the cluster up to the point
#	        where kubectl can take over (can be used with minikube)
#

CLUSTER_TYPE="gke"

#
# Uncomment and set to use a specific GKE project id instead of the default
#
# GKE_PROJECT_ID="<my-project-id>"

#
# Uncomment and set to use a specific region
#
# GKE_COMPUTE_REGION="us-west1-a"

#
# How many nodes should the cluster start with. Too few nodes, and 
# (depending on the pods/node value) some pods won't start. "Initial"
# here means that the count can change based on whether autoscaling
# is enabled, but autoscaling is not handled here.
#

GKE_INITIAL_NODE_COUNT=4

#
# Docker builds are somewhat memory hungry (see deployment-worker.yaml).
# Ideally, one would use a heterogeneous cluster, with node pools tailored
# to pods' needs. This may or may not be possible. The page at 
# https://cloud.google.com/kubernetes-engine/docs/how-to/node-pools suggests
# that it may be possible to add node pools of various forms to an existing
# cluster. For now, we work with simple homogeneous clusters.
#

GKE_MACHINE_TYPE=n1-highmem-2

#
# VM type to use with minikube
#

MINIKUBE_VM_TYPE="virtualbox"

#
# Change to use a custom cluster name
#

CLUSTER_NAME="wholetale"

############################################################
#                                                          #
#                       Volumes                            #
#                                                          #
############################################################
#
# Volumes are created automatically using a persistent volume 
# claim which lets Kubernetes dynamically provision the volumes.
# For custom volumes (such as local volumes that need not be
# persisted across pod reschedules or for using a fixed persistent
# GCE disk, use custom taml/yaml specs

VOLSZ_MONGO_CFG="20Mi"

#
# Size of mongo storage volume
#

VOLSZ_MONGO_DATA="200Mi"

#
# Size of registry storage
#

VOLSZ_REGISTRY_STORAGE="4Gi"

#
# Size of WholeTale physical storage (for caching datasets)
#

VOLSZ_GIRDER_PS="200Mi"

#
# Volume used for storing layers for image builds
#

VOLSZ_DOCKER_STORAGE="10Gi"

#
# Volume used to assemble build contexts for repo2docker
#

VOLSZ_IMAGE_BUILD_DIRS="4Gi"

#
# Volumes for tale instance workspace. In this naive setup,
# each instance gets a fixed volume of the specified size.
#

VOLSZ_WORKSPACE="1Gi"

#
# The domain name of the cluster to configure ingress
#

DOMAIN_NAME=local.wholetale.org

#
# Globus app secrets
#

GLOBUS_CLIENT_ID="3a3d537c-9ac5-4e9d-8790-d92dd5eb3b07"
GLOBUS_CLIENT_SECRET="JATkgV1xR2ROlrGdDhEbHaZY65H6UTOHEJ3aLKJfBnk="

#
# Images to use
# 

GIRDER_IMAGE=wholetale/girder:latest
DASHBOARD_IMAGE=wholetale/dashboard:latest
MONGO_IMAGE=mongo:3.2
REDIS_IMAGE=redis
REGISTRY_IMAGE=registry:2.6
WORKER_IMAGE=hategan/wt-gwvolman:latest
GIRDERFS_DRIVER_IMAGE=hategan/wt-girderfs-flexvolume:0.0.1
